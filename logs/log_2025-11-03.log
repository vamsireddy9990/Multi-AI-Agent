2025-11-03 00:13:09,287 - INFO - starting backend service..
2025-11-03 00:13:11,289 - INFO - Starting Frontend service
2025-11-03 15:09:05,606 - INFO - starting backend service..
2025-11-03 15:09:07,607 - INFO - Starting Frontend service
2025-11-03 15:12:32,982 - INFO - starting backend service..
2025-11-03 15:12:34,983 - INFO - Starting Frontend service
2025-11-03 15:17:35,554 - INFO - starting backend service..
2025-11-03 15:17:37,555 - INFO - Starting Frontend service
2025-11-03 15:23:07,872 - INFO - starting backend service..
2025-11-03 15:23:09,872 - INFO - Starting Frontend service
2025-11-03 15:24:47,383 - INFO - starting backend service..
2025-11-03 15:24:49,383 - INFO - Starting Frontend service
2025-11-03 15:35:24,659 - INFO - starting backend service..
2025-11-03 15:35:26,660 - INFO - Starting Frontend service
2025-11-03 17:00:51,449 - INFO - starting backend service..
2025-11-03 17:00:53,450 - INFO - Starting Frontend service
2025-11-03 17:03:04,338 - INFO - Sending request to backend
2025-11-03 17:03:04,371 - INFO - Received request for model : llama3-70b-8192
2025-11-03 17:03:05,541 - ERROR - Some error ocuured during reponse generation
2025-11-03 17:03:05,544 - ERROR - Backend error
2025-11-03 17:34:40,247 - INFO - starting backend service..
2025-11-03 17:34:42,248 - INFO - Starting Frontend service
2025-11-03 17:35:06,265 - INFO - Sending request to backend
2025-11-03 17:35:06,304 - INFO - Received request for model : llama3-70b-8192
2025-11-03 17:35:09,161 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-03 17:35:09,162 - ERROR - Some error ocuured during reponse generation
2025-11-03 17:35:09,165 - ERROR - Backend error
2025-11-03 17:38:01,583 - INFO - Sending request to backend
2025-11-03 17:38:01,587 - INFO - Received request for model : llama3-70b-8192
2025-11-03 17:38:03,457 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-03 17:38:03,458 - ERROR - Some error ocuured during reponse generation
2025-11-03 17:38:03,460 - ERROR - Backend error
2025-11-03 17:38:05,103 - INFO - Sending request to backend
2025-11-03 17:38:05,112 - INFO - Received request for model : llama3-70b-8192
2025-11-03 17:38:06,415 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-03 17:38:06,417 - ERROR - Some error ocuured during reponse generation
2025-11-03 17:38:06,419 - ERROR - Backend error
2025-11-03 17:38:27,118 - INFO - Sending request to backend
2025-11-03 17:38:27,124 - INFO - Received request for model : llama3-70b-8192
2025-11-03 17:38:28,499 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-03 17:38:28,500 - ERROR - Some error ocuured during reponse generation
2025-11-03 17:38:28,503 - ERROR - Backend error
2025-11-03 17:40:56,181 - INFO - starting backend service..
2025-11-03 17:40:58,182 - INFO - Starting Frontend service
2025-11-03 17:42:03,651 - INFO - Sending request to backend
2025-11-03 17:42:03,680 - INFO - Received request for model : llama3-70b-8192
2025-11-03 17:42:06,748 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-03 17:42:06,749 - ERROR - Some error occurred during response generation. See traceback below.
Traceback (most recent call last):
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\app\backend\api.py", line 29, in chat_endpoint
    response = get_response_from_ai_agents(
        request.model_name,
    ...<2 lines>...
        request.system_prompt
    )
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\app\core\ai_agent.py", line 22, in get_response_from_ai_agents
    response = agent.invoke(state)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\main.py", line 3094, in invoke
    for chunk in self.stream(
                 ~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<10 lines>...
        **kwargs,
        ^^^^^^^^^
    ):
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\main.py", line 2679, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        [t for t in loop.tasks.values() if not t.writes],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        schedule_task=loop.accept_push,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\_runner.py", line 167, in tick
    run_with_retry(
    ~~~~~~~~~~~~~~^
        t,
        ^^
    ...<10 lines>...
        },
        ^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain\agents\factory.py", line 1065, in model_node
    response = _execute_model_sync(request)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain\agents\factory.py", line 1038, in _execute_model_sync
    output = model_.invoke(messages)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\runnables\base.py", line 5489, in invoke
    return self.bound.invoke(
           ~~~~~~~~~~~~~~~~~^
        input,
        ^^^^^^
        self._merge_configs(config),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **{**self.kwargs, **kwargs},
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 379, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1088, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 903, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1192, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_groq\chat_models.py", line 544, in _generate
    response = self.client.create(messages=message_dicts, **params)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\groq\resources\chat\completions.py", line 464, in create
    return self._post(
           ~~~~~~~~~~^
        "/openai/v1/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<45 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'The model `llama3-70b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
During task with name 'model' and id 'cab56c1f-89b4-8eb7-7a10-3451cf690774'
2025-11-03 17:42:07,003 - ERROR - Backend error
2025-11-03 17:58:15,917 - INFO - starting backend service..
2025-11-03 17:58:17,918 - INFO - Starting Frontend service
2025-11-03 17:58:43,333 - INFO - Sending request to backend
2025-11-03 17:58:43,350 - INFO - Received request for model : llama3-70b-8192
2025-11-03 17:58:45,148 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-03 17:58:45,150 - ERROR - Some error occurred during response generation. See traceback below.
Traceback (most recent call last):
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\app\backend\api.py", line 29, in chat_endpoint
    response = get_response_from_ai_agents(
        request.model_name,
    ...<2 lines>...
        request.system_prompt
    )
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\app\core\ai_agent.py", line 22, in get_response_from_ai_agents
    response = agent.invoke(state)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\main.py", line 3094, in invoke
    for chunk in self.stream(
                 ~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<10 lines>...
        **kwargs,
        ^^^^^^^^^
    ):
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\main.py", line 2679, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        [t for t in loop.tasks.values() if not t.writes],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        schedule_task=loop.accept_push,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\_runner.py", line 167, in tick
    run_with_retry(
    ~~~~~~~~~~~~~~^
        t,
        ^^
    ...<10 lines>...
        },
        ^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain\agents\factory.py", line 1065, in model_node
    response = _execute_model_sync(request)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain\agents\factory.py", line 1038, in _execute_model_sync
    output = model_.invoke(messages)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\runnables\base.py", line 5489, in invoke
    return self.bound.invoke(
           ~~~~~~~~~~~~~~~~~^
        input,
        ^^^^^^
        self._merge_configs(config),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **{**self.kwargs, **kwargs},
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 379, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1088, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 903, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1192, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_groq\chat_models.py", line 544, in _generate
    response = self.client.create(messages=message_dicts, **params)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\groq\resources\chat\completions.py", line 464, in create
    return self._post(
           ~~~~~~~~~~^
        "/openai/v1/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<45 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'The model `llama3-70b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
During task with name 'model' and id '8f588dc5-e91a-df06-f26b-5a453c52ba2e'
2025-11-03 17:58:45,188 - ERROR - Backend error
2025-11-03 18:03:33,088 - INFO - starting backend service..
2025-11-03 18:03:35,089 - INFO - Starting Frontend service
2025-11-03 18:04:02,431 - INFO - Sending request to backend
2025-11-03 18:04:04,484 - ERROR - Error occured while sending request to backend
2025-11-03 18:09:52,080 - INFO - Sending request to backend
2025-11-03 18:09:54,106 - ERROR - Failed to connect to the backend server at http://127.0.0.1:9999/chat. Ensure uvicorn is running.
2025-11-03 18:11:23,282 - INFO - Sending request to backend
2025-11-03 18:11:25,316 - ERROR - Failed to connect to the backend server at http://127.0.0.1:9999/chat. Ensure uvicorn is running.
2025-11-03 18:13:21,147 - INFO - Sending request to backend
2025-11-03 18:13:21,183 - INFO - Received request for model : llama3-8b-8192
2025-11-03 18:13:23,279 - ERROR - Some error occurred during response generation. See traceback below.
Traceback (most recent call last):
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\app\backend\api.py", line 29, in chat_endpoint
    response = get_response_from_ai_agents(
        request.model_name,
    ...<2 lines>...
        request.system_prompt
    )
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\app\core\ai_agent.py", line 20, in get_response_from_ai_agents
    agent = create_agent(
        model=llm,
        tools=tools,
        state_modifier=system_prompt
    )
TypeError: create_agent() got an unexpected keyword argument 'state_modifier'
2025-11-03 18:13:23,288 - ERROR - Backend HTTP error: Failed to get AI response | Error: create_agent() got an unexpected keyword argument 'state_modifier' | File: C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\app\backend\api.py | Line: 29
2025-11-03 18:16:41,236 - INFO - Sending request to backend
2025-11-03 18:16:41,267 - INFO - Received request for model : llama3-8b-8192
2025-11-03 18:16:44,541 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-03 18:16:44,543 - ERROR - Some error occurred during response generation. See traceback below.
Traceback (most recent call last):
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\app\backend\api.py", line 29, in chat_endpoint
    response = get_response_from_ai_agents(
        request.model_name,
    ...<2 lines>...
        request.system_prompt
    )
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\app\core\ai_agent.py", line 35, in get_response_from_ai_agents
    response = agent.invoke(state)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\main.py", line 3094, in invoke
    for chunk in self.stream(
                 ~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<10 lines>...
        **kwargs,
        ^^^^^^^^^
    ):
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\main.py", line 2679, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        [t for t in loop.tasks.values() if not t.writes],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        schedule_task=loop.accept_push,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\_runner.py", line 167, in tick
    run_with_retry(
    ~~~~~~~~~~~~~~^
        t,
        ^^
    ...<10 lines>...
        },
        ^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain\agents\factory.py", line 1065, in model_node
    response = _execute_model_sync(request)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain\agents\factory.py", line 1038, in _execute_model_sync
    output = model_.invoke(messages)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\runnables\base.py", line 5489, in invoke
    return self.bound.invoke(
           ~~~~~~~~~~~~~~~~~^
        input,
        ^^^^^^
        self._merge_configs(config),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **{**self.kwargs, **kwargs},
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 379, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1088, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 903, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1192, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\langchain_groq\chat_models.py", line 544, in _generate
    response = self.client.create(messages=message_dicts, **params)
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\groq\resources\chat\completions.py", line 464, in create
    return self._post(
           ~~~~~~~~~~^
        "/openai/v1/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<45 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
During task with name 'model' and id '7e9c4bfe-8e51-63fe-cec3-f86230cb3365'
2025-11-03 18:16:44,586 - ERROR - Backend HTTP error: Failed to get AI response | Error: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}} | File: C:\Users\Vamsi Reddy\Downloads\multiAI_Agent\app\backend\api.py | Line: 29
2025-11-03 18:25:49,394 - INFO - Sending request to backend
2025-11-03 18:25:49,422 - INFO - Received request for model : llama-3.1-8b-instant
2025-11-03 18:25:52,483 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:25:56,475 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:25:56,478 - INFO - Sucesfully got response from AI Agent llama-3.1-8b-instant
2025-11-03 18:25:56,484 - INFO - Successfully received response from backend
2025-11-03 18:26:43,157 - INFO - Sending request to backend
2025-11-03 18:26:43,164 - INFO - Received request for model : llama-3.1-8b-instant
2025-11-03 18:26:45,933 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:26:51,038 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:26:51,041 - INFO - Sucesfully got response from AI Agent llama-3.1-8b-instant
2025-11-03 18:26:51,044 - INFO - Successfully received response from backend
2025-11-03 18:43:20,646 - INFO - Sending request to backend
2025-11-03 18:43:20,672 - INFO - Received request for model : llama-3.1-8b-instant
2025-11-03 18:43:27,124 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:43:31,394 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:43:36,020 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-11-03 18:43:36,021 - INFO - Retrying request to /openai/v1/chat/completions in 8.000000 seconds
2025-11-03 18:43:47,034 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:43:47,038 - INFO - Sucesfully got response from AI Agent llama-3.1-8b-instant
2025-11-03 18:43:47,041 - INFO - Successfully received response from backend
2025-11-03 18:44:02,961 - INFO - Sending request to backend
2025-11-03 18:44:02,969 - INFO - Received request for model : llama-3.1-8b-instant
2025-11-03 18:44:06,275 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:44:10,821 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-11-03 18:44:10,822 - INFO - Retrying request to /openai/v1/chat/completions in 29.000000 seconds
2025-11-03 18:44:41,234 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:44:41,237 - INFO - Sucesfully got response from AI Agent llama-3.1-8b-instant
2025-11-03 18:44:41,239 - INFO - Successfully received response from backend
2025-11-03 18:45:06,771 - INFO - Sending request to backend
2025-11-03 18:45:06,783 - INFO - Received request for model : llama-3.1-8b-instant
2025-11-03 18:45:09,393 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:45:17,995 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-11-03 18:45:17,996 - INFO - Retrying request to /openai/v1/chat/completions in 31.000000 seconds
2025-11-03 18:45:50,762 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-03 18:45:50,766 - INFO - Sucesfully got response from AI Agent llama-3.1-8b-instant
2025-11-03 18:45:50,769 - INFO - Successfully received response from backend
2025-11-03 22:00:07,940 - INFO - starting backend service..
2025-11-03 22:00:09,941 - INFO - Starting Frontend service
